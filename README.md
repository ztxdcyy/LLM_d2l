动手学LLM

Todo顺序：Llama Transformer -> FlashAttention -> llama.cpp -> 量化 

- [ ] llama系列
  - [ ] Transformer
    - [x] MHA
    - [x] RoPE  （有一个版本的理解，但是不会写，还是不够懂）
    ![alt text](img/image.png)
    - [ ] KVcache
    - [ ] RMSNorm
  - [ ] llama.cpp
  - [ ] llama2
    - [ ] GQA
  - [ ] llama3.1
    - [ ] 暂时不了解有哪些改进

- [ ] FlashAttention
  - [ ] python
  - [ ] cpp
  - [ ] cuda

- [ ] 量化


